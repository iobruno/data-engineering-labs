{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232d3f5c-f2dd-481f-ae8e-ef094e730a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import col, dayofmonth, month, count, dense_rank, lit, broadcast, max as spark_max\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a62ca-14ce-448e-bb21-b4a8c4bb173d",
   "metadata": {},
   "source": [
    "## Spark Connect Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc346ebf-2848-42f6-9613-cf810673ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.remote(\"sc://localhost\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfc512-824a-4c99-af3c-66f384aba4f9",
   "metadata": {},
   "source": [
    "## Load Datasets from GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfc13a3-43d9-491d-abf5-fd00e3d55214",
   "metadata": {},
   "source": [
    "### FHV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22456092-e53c-4df1-baf7-f78861433a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_fhv = spark.read.parquet(\"gs://iobruno-lakehouse-raw/nyc_tlc_dataset/fhv_trip_data/fhv_tripdata_2019-10.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8d1eb8-150f-4dd2-b63e-c1f54db5c662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   NULL|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   NULL|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   NULL|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|          82|          82|   NULL|       B00021         |\n",
      "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|         264|          71|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|         264|          39|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|         264|         188|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|         264|          91|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|         264|          71|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|         264|          35|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|         264|          61|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|         264|         198|   NULL|                B00037|\n",
      "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|         264|         264|   NULL|                  NULL|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "raw_fhv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7tyc3qahb5p",
   "metadata": {},
   "source": [
    "### Performance Optimization: Caching\n",
    "\n",
    "The `fhv` DataFrame is used multiple times throughout this notebook (for counting, joins, aggregations, and writing).  \n",
    "Calling `.cache()` stores the DataFrame in memory after its first computation, avoiding repeated reads from GCS on subsequent actions. The `.count()` triggers the caching immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f52658lfmed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1897493"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fhv = raw_fhv.select(\n",
    "    col('dispatching_base_num'),\n",
    "    col('Affiliated_base_number').alias('affiliated_base_num'),\n",
    "    col('PUlocationID').alias('pickup_location_id'),\n",
    "    col('DOlocationID').alias('dropoff_location_id'),\n",
    "    col('pickup_datetime').cast('timestamp'),\n",
    "    col('dropOff_datetime').cast('timestamp').alias('dropoff_datetime'),\n",
    "    col('SR_Flag').alias('sr_flag'),\n",
    ").cache()\n",
    "\n",
    "fhv.count()  # Trigger caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48b25d-8567-4931-b853-dc00ccb8dc0d",
   "metadata": {},
   "source": [
    "### Zone Lookup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84329e08-b7f0-4b41-9b85-0cf08ef8c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_schema = StructType([\n",
    "    StructField(\"LocationID\", IntegerType(), True),\n",
    "    StructField(\"Borough\", StringType(), True),\n",
    "    StructField(\"Zone\", StringType(), True),\n",
    "    StructField(\"service_zone\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda76359-88cc-4b32-a7c7-ff3b48c51097",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = spark.read.csv(\n",
    "    path=\"gs://iobruno-lakehouse-raw/nyc_tlc_dataset/zone_lookup/*.csv.gz\",\n",
    "    header=True,\n",
    "    inferSchema=False,\n",
    "    schema=zones_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb0be05b-f47d-4044-87f1-c8227bcef1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "139a31ea-2546-4b47-b728-172d5df9b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = zones.select(\n",
    "    col('LocationID').alias('location_id'),\n",
    "    col('Borough').alias('borough'),\n",
    "    col('Zone').alias('zone'),\n",
    "    col('service_zone')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca37d5-a578-4572-9713-f11880cf8239",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d835f4-cba9-40a9-b499-930ebb7585f9",
   "metadata": {},
   "source": [
    "**FHV October 2019**  \n",
    "- Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons\n",
    "- Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)?  Select the answer which most closely matches\n",
    "- [ ] 1 MB\n",
    "- [x] 6 MB\n",
    "- [ ] 25 MB\n",
    "- [ ] 87 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swlt816gjg",
   "metadata": {},
   "source": [
    "### Performance Optimization: coalesce(n) vs repartition(n)\n",
    "\n",
    "When reducing the number of partitions, `coalesce(n)` is more efficient than `repartition(n)`.  \n",
    "Coalesce avoids a full shuffle by combining existing partitions locally, while repartition always performs a full shuffle of all data across the cluster.\n",
    "\n",
    "Note: `coalesce` may result in uneven partition sizes; use `repartition` if even distribution is critical for downstream operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16b75697-43b7-40fd-8425-d1d660d8a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv.coalesce(6) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"gs://iobruno-lakehouse-raw/tmp_output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a31ba-76c8-4964-a4c1-ba1279eea57c",
   "metadata": {},
   "source": [
    "**Count records**  \n",
    "\n",
    "How many taxi trips were there on the 15th of October?  \n",
    "Consider only trips that started on the 15th of October\n",
    "\n",
    "- [ ] 108,164\n",
    "- [ ] 12,856\n",
    "- [ ] 452,470\n",
    "- [x] 62,610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9d6d612-4f82-4a2e-af90-9ada87b4d359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(num_trips=62610)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fhv.filter(\n",
    "    (month(col(\"pickup_datetime\")) == 10) &\n",
    "    (dayofmonth(col(\"pickup_datetime\")) == 15)\n",
    ").agg(count(lit(1)).alias(\"num_trips\")).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e0205-73fb-4462-b280-b71bb74cc680",
   "metadata": {},
   "source": [
    "**Longest trip for each day**  \n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?\n",
    "- [x] 631,152.50 Hours\n",
    "- [ ] 243.44 Hours\n",
    "- [ ] 7.68 Hours\n",
    "- [ ] 3.32 Hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30yr0ipinef",
   "metadata": {},
   "source": [
    "### Performance Optimization: max( ) vs Window function\n",
    "\n",
    "Using `spark_max()` (aliased from `max`) is more efficient than a window function with `dense_rank()` when finding the maximum value.  \n",
    "Window functions require sorting all data to a single partition (shown in the warning), while `max()` can compute in parallel across partitions and only combine the partial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0792b43b-36a6-463a-ae42-02c316d5cc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max_duration_hours=631152.5)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fhv.agg(\n",
    "    spark_max((col(\"dropoff_datetime\").cast(\"long\") - col(\"pickup_datetime\").cast(\"long\")) / 3600).alias(\"max_duration_hours\")\n",
    ").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e08bed-582a-4aa6-8537-9ddad4b513c8",
   "metadata": {},
   "source": [
    "**Least frequent pickup location zone**\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark [Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)  \n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?\n",
    "\n",
    "- [ ] East Chelsea\n",
    "- [x] Jamaica Bay\n",
    "- [ ] Union Sq\n",
    "- [ ] Crown Heights North"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "udd201sqt2",
   "metadata": {},
   "source": [
    "### Performance Optimization: Broadcast Join\n",
    "\n",
    "The `zones` lookup table is small (~265 rows) compared to the `fhv` dataset (1.9M rows).  When joining these tables, Spark would normally shuffle the large dataset across the cluster.   \n",
    "Using `broadcast(zones)` sends the small `zones` table to all worker nodes, allowing the join to happen locally without shuffling the large dataset. This is much faster than a standard shuffle join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15ab7cf3-48f4-471b-9164-6568899c3ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iobruno/Vault/data-engineering-labs/module5-batch-processing/pyspark/.venv/lib/python3.13/site-packages/pyspark/sql/connect/expressions.py:1091: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(zone='Jamaica Bay', num_trips=1, rnk=1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_count_window = Window.orderBy(col(\"num_trips\").asc())\n",
    "\n",
    "trips_per_location = (\n",
    "    fhv\n",
    "    .groupBy(\"pickup_location_id\")\n",
    "    .agg(count(lit(1)).alias(\"num_trips\"))\n",
    "    .withColumn(\"rnk\", dense_rank().over(trips_count_window))\n",
    ")\n",
    "\n",
    "trips_per_location \\\n",
    "    .join(broadcast(zones), trips_per_location.pickup_location_id == zones.location_id, \"inner\") \\\n",
    "    .filter(col(\"rnk\") == 1) \\\n",
    "    .select(col(\"zone\"), col(\"num_trips\"), col(\"rnk\")) \\\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620e627-c2c8-4dda-9d05-d6ab51fa647f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
