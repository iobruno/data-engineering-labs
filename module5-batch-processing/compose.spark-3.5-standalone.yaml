x-spark-image: &spark-image spark:${SPARK_VERSION:-3.5.7-scala2.12-java17-python3-ubuntu}

x-spark-common:
  &spark-common
  image: *spark-image
  environment:
    &spark-common-env
    SPARK_NO_DAEMONIZE: true    # Forces the process to run in foreground (req. for Docker)
    SPARK_PUBLIC_DNS: localhost  # Ensures Web UI links point to localhost instead of container IPs
    GOOGLE_APPLICATION_CREDENTIALS: "/secrets/gcp_credentials.json"
  volumes:
    &spark-common-vol
    - ./logs/:/opt/spark/logs/
    - ./spark-3.5-standalone.conf:/opt/spark/conf/spark-standalone.conf
    - ~/.gcp/spark_credentials.json:/secrets/gcp_credentials.json
    - vol-spark-extra-jars:/opt/spark/extra-jars/
  depends_on:
    &spark-common-depends-on
    spark-init:
      condition: service_completed_successfully

services:
  spark-master:
    <<: *spark-common
    container_name: spark-master
    command: |
      /opt/spark/sbin/start-master.sh
      --properties-file /opt/spark/conf/spark-standalone.conf
      --webui-port 4040
    ports:
      - '4040:4040'   # default: 8080 for master
      - '7077:7077'
    restart: on-failure:5

  spark-worker1:
    <<: *spark-common
    container_name: spark-worker1
    command: > 
      /opt/spark/sbin/start-worker.sh 
      spark://spark-master:7077 
      --cores 2
      --memory 4G
      --properties-file /opt/spark/conf/spark-standalone.conf
      --webui-port 4041
    ports:
      - '4041:4041'   # default: 8081 for worker
    depends_on:
      spark-master:
        condition: service_started
    restart: on-failure:5

  spark-worker2:
    <<: *spark-common
    container_name: spark-worker2
    command: > 
      /opt/spark/sbin/start-worker.sh 
      spark://spark-master:7077 
      --cores 4
      --memory 6G
      --properties-file /opt/spark/conf/spark-standalone.conf
      --webui-port 4042
    ports:
      - '4042:4042'   # default: 8081 for worker
    depends_on:
      spark-master:
        condition: service_started
    restart: on-failure:5

  spark-connect:
    <<: *spark-common
    container_name: spark-connect
    command: |
      /opt/spark/sbin/start-connect-server.sh
      --master spark://spark-master:7077
      --properties-file /opt/spark/conf/spark-standalone.conf
      --packages org.apache.spark:spark-connect_2.12:3.5.7
      --conf spark.jars.ivy=/tmp/.ivy2
    ports:
      - '15002:15002'
    depends_on:
      spark-master:
        condition: service_started
    restart: on-failure:5

  spark-history-server:
    <<: *spark-common
    container_name: spark-history-server
    command: |
      /opt/spark/sbin/start-history-server.sh
      --properties-file /opt/spark/conf/spark-standalone.conf
    environment:
      <<: *spark-common-env
      SPARK_HISTORY_OPTS: >-
        -Dspark.history.fs.logDirectory=/opt/spark/logs/
    ports:
      - '18080:18080'
    depends_on:
      spark-master:
        condition: service_started
    restart: on-failure:5

  spark-init:
    image: *spark-image
    container_name: spark-init
    user: 0:0
    entrypoint: /bin/bash
    command:
      - -c
      - |
        apt-get update && apt-get install curl -y
        curl --create-dirs -O --output-dir /opt/spark/extra-jars/ https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.32/gcs-connector-hadoop3-2.2.32-shaded.jar
        chown -R 185:185 /opt/spark/extra-jars/
    volumes:
      - vol-spark-extra-jars:/opt/spark/extra-jars/

volumes:
  vol-spark-extra-jars:
    name: vol-spark-extra-jars